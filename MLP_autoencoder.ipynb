{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP_autoencoder.ipynb","version":"0.3.2","provenance":[{"file_id":"1ZGScv51DC7TdNJcC24jXgyWuXyRTn0UL","timestamp":1556092366028}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"W7zsPERtr0LC","colab_type":"text"},"cell_type":"markdown","source":["# Autoencoder"]},{"metadata":{"id":"9iL5KH63r0LE","colab_type":"code","colab":{}},"cell_type":"code","source":["# If pytorch is not installed, uncommand and run the following line to install pytorch\n","#!pip install torch torchvision"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YbUmP-VGVvcP","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QoPiqjikr0LT","colab_type":"text"},"cell_type":"markdown","source":["**numpy**는 다차원 배열 및 벡터/ 행렬 기본 연산\n","python으로 data science를 할 때 가장 기본이 되는 라이브러리 중 하나."]},{"metadata":{"id":"kjpK8daar0LK","colab_type":"code","outputId":"7a1b20a9-2de4-44e2-cf47-21c60312fe5e","executionInfo":{"status":"ok","timestamp":1556096167874,"user_tz":-540,"elapsed":1422,"user":{"displayName":"Injung Kim","photoUrl":"","userId":"13406367766306429798"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["import torch\n","import torchvision            \n","import torch.nn as nn\n","\n","print(torch.__version__)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["1.0.1.post2\n"],"name":"stdout"}]},{"metadata":{"id":"LhNkRvYkr0LN","colab_type":"text"},"cell_type":"markdown","source":["**torch**: pytorch package\n","\n","**torch.nn**: 신경망 모델에 Class들을 포함\n","\n","**torchvision**은 computer vision에 많이 사용되는 dataset, model, transform을 포함 (https://pytorch.org/docs/stable/torchvision/index.html)"]},{"metadata":{"id":"ntWD5II587og","colab_type":"code","colab":{}},"cell_type":"code","source":["from torch.utils.data import DataLoader"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YgsGVrkrAEPD","colab_type":"text"},"cell_type":"markdown","source":["Data Loader: 데이터 로드를 위한 패키지 (Dataset + Sampler + Iterator)\n","> * Dataset is an abstract class representing a dataset \n","> * Sampler provides a way to iterate over indices of dataset elements\n","\n","\n","See https://pytorch.org/docs/stable/data.html"]},{"metadata":{"id":"_tUvuvvPr0LO","colab_type":"code","colab":{}},"cell_type":"code","source":["from torchvision import datasets\n","from torchvision import transforms"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Odn-STAQAmKq","colab_type":"text"},"cell_type":"markdown","source":["**dataset**: MNIST, fashion MNIST, COCO, LSUN, CIFAR, etc.\n","\n","**transforms**: algorithms for preprocessing or data augmentation\n","\n","See https://pytorch.org/docs/stable/torchvision/index.html to know datasets and transforms in torchvision"]},{"metadata":{"id":"mtKXCay2kN5Z","colab_type":"text"},"cell_type":"markdown","source":["# Using (deep) neural networks with python\n","\n","\n","1. Define a network model\n","\n","2. Prepare data\n","\n","3. Train the model\n","\n","4. Evalute the model"]},{"metadata":{"id":"eCc7O8M5r0LT","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","\n","#import matplotlib\n","\n","from matplotlib.pyplot import imshow, imsave"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qgaevoCUr0LW","colab_type":"text"},"cell_type":"markdown","source":["**matplotlib**: python visualization library"]},{"metadata":{"id":"hYk9uq_Br0LW","colab_type":"code","outputId":"58e329c2-635a-405d-f68e-c753c4a94623","executionInfo":{"status":"ok","timestamp":1556096168216,"user_tz":-540,"elapsed":1716,"user":{"displayName":"Injung Kim","photoUrl":"","userId":"13406367766306429798"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"cell_type":"code","source":["MODEL_NAME = 'MLP'\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"MODEL_NAME = {}, DEVICE = {}\".format(MODEL_NAME, DEVICE))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["MODEL_NAME = MLP, DEVICE = cuda\n"],"name":"stdout"}]},{"metadata":{"id":"AhUqWS9-r0La","colab_type":"text"},"cell_type":"markdown","source":["GPU가 있다면 GPU를 통해 학습을 가속화하고, 없으면 CPU로 학습하기 위해 device를 정해준다.\n","\n","**torch.cuda.is_avaliable()**은 GPU가 사용가능한지를 판단하는 함수"]},{"metadata":{"id":"8ljFCGEIXfCy","colab_type":"text"},"cell_type":"markdown","source":["## Defining a Neural Network model using pytorch\n","\n","1. Define a neural net model\n","\n","> * Define a model class inheriting **nn.module**\n","\n",">> nn.module is the base class of all layers/operators\n","\n",">* Define **__init__** function (constructor)\n","\n",">>  Create layers and operators\n","\n",">* Define **forward** function (forward propagation)\n","\n",">> Define how to compute the output from the input\n","\n","> Example\n","\n","~~~~\n","    class Model(nn.Module):\n","        def __init__(self):\n","            super(Model, self).__init__()\n","            self.conv1 = nn.Conv2d(1, 20, 5)\n","            self.conv2 = nn.Conv2d(20, 20, 5)           \n","                   \n","        def forward(self, x):\n","            x = F.relu(self.conv1(x))\n","            return F.relu(self.conv2(x))\n","~~~~\n","\n","> Note! You don't need to backpropagation procedure, because pytorch provides **autograd**"]},{"metadata":{"id":"sbpvoKXzr0Lb","colab_type":"code","colab":{}},"cell_type":"code","source":["class HelloMLP(nn.Module):\n","    def __init__(self, input_size=784, hidden_size=200):\n","        super(HelloMLP, self).__init__()\n","        middle_size = int((input_size + hidden_size) / 2);\n","        self.mlp = nn.Sequential(             # a sequential container\n","            # 1st layer\n","            nn.Linear(input_size, middle_size),        # matrix multiplication (fully connected layer)            \n","            nn.Tanh(),                        # activation function (nn.ReLU(), nn.Tanh(), nn.Sigmoid(), etc.)\n","            \n","            # 2nd layer\n","            nn.Linear(middle_size, hidden_size),                # matrix multiplication (fully connected layer)\n","            nn.Tanh(),                        # activation function\n","            \n","            # 3rd layer\n","            nn.Linear(hidden_size, middle_size),\n","            nn.Tanh(),\n","            \n","            # 3rd layer\n","            nn.Linear(middle_size, input_size),\n","        )\n","    \n","    def forward(self, x):\n","        y_ = x.view(-1, 28*28)            # Reshape input tensor (N, 28, 28) --> (N, 28, 28)\n","        y_ = self.mlp(y_)                     # compute \n","        y_ = y_.view(-1, 1, 28, 28)\n","        return y_"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BfM4wnwdr0Le","colab_type":"text"},"cell_type":"markdown","source":["**nn.Sequential()**: a sequential container.\n","\n","* Example of using Sequential\n","\n","~~~~\n","  model = nn.Sequential(\n","    nn.Conv2d(1,20,5),\n","    nn.ReLU(),\n","    nn.Conv2d(20,64,5),\n","    nn.ReLU()\n","    )\n","~~~~\n","* Example of using Sequential with OrderedDict\n","\n","~~~~\n","  model = nn.Sequential(OrderedDict([\n","    ('conv1', nn.Conv2d(1,20,5)),\n","    ('relu1', nn.ReLU()),\n","    ('conv2', nn.Conv2d(20,64,5)),\n","    ('relu2', nn.ReLU())\n","    ]))\n","~~~~\n","\n","**nn.ModuleList()**: a list-like container class \n","\n","* Example of using ModuleList\n","\n","~~~~\n","  class MyModule(nn.Module):\n","      def __init__(self):\n","          super(MyModule, self).__init__()\n","          self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])\n","            \n","      def forward(self, x):\n","          # ModuleList can act as an iterable, or be indexed using ints\n","          for i, l in enumerate(self.linears):\n","               x = self.linears[i // 2](x) + l(x)\n","          return x\n","                 \n","~~~~"]},{"metadata":{"id":"fyP-DP1Sr0Lh","colab_type":"code","colab":{}},"cell_type":"code","source":["model = HelloMLP().to(DEVICE)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cyyU-712c1Np","colab_type":"text"},"cell_type":"markdown","source":["Moves and/or casts the parameters and buffers. (CPU or GPU)"]},{"metadata":{"id":"Z9-QvNvbksQj","colab_type":"text"},"cell_type":"markdown","source":["## Loading and preprocessing data\n","\n"]},{"metadata":{"id":"Glhmh_OHlBMY","colab_type":"text"},"cell_type":"markdown","source":["Transform of input data"]},{"metadata":{"id":"NIWGKzSbbE23","colab_type":"code","colab":{}},"cell_type":"code","source":["# mean and stdev of MNIST\n","mean = 0.1307\n","stdev = 0.3081"],"execution_count":0,"outputs":[]},{"metadata":{"id":"oq2EYzdBr0Lk","colab_type":"code","colab":{}},"cell_type":"code","source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),                               # image to tensor\n","     transforms.Normalize(mean=(mean,), std=(stdev,))  # normalize to \"(x-mean)/std\"\n","    ])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2hE6ydS3r0Lm","colab_type":"text"},"cell_type":"markdown","source":["**transforms**: torchvision에서 제공하는 transform 함수들이 있는 패키지.\n","\n","**ToTensor**: numpy array를 torch tensor로 변환.\n","\n","**Normalize**: 정규화 함수 output[channel] = (input[channel] - mean[channel]) / std[channel]"]},{"metadata":{"id":"buibF906r0Lm","colab_type":"code","colab":{}},"cell_type":"code","source":["mnist_train = datasets.MNIST(root='../data/', train=True, transform=transform, download=True)\n","mnist_test = datasets.MNIST(root='../data/', train=False, transform=transform, download=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lYL_NEavr0Lo","colab_type":"text"},"cell_type":"markdown","source":["**datasets**에는 여러 데이터들에 대해 다운로드하고 처리하는 클래스가 내장되어 있음. [참고](https://pytorch.org/docs/stable/torchvision/datasets.html)\n","\n","root 폴더에 없을 시에 download하고, 앞서 정의한 transform에 따라 전처리 된 데이터를 return함."]},{"metadata":{"id":"wGbeeCkCr0Lp","colab_type":"code","colab":{}},"cell_type":"code","source":["batch_size = 64"],"execution_count":0,"outputs":[]},{"metadata":{"id":"6_n0AFCmr0Lr","colab_type":"code","colab":{}},"cell_type":"code","source":["train_loader = DataLoader(dataset=mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n","test_loader = DataLoader(dataset=mnist_test, batch_size=100, shuffle=False, drop_last=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GR9GEof0r0Ls","colab_type":"text"},"cell_type":"markdown","source":["**DataLoader**는 pytorch에서 학습 시에 데이터를 배치 사이즈만큼씩 효율적으로 불러오도록 돕는 클래스. 잘 사용할수록 GPU의 사용률이 올라간다.\n","\n","**shuffle**: every epochs 마다 데이터의 순서를 랜덤하게 섞는다.\n","\n","**drop_last**: 데이터의 개수가 배치 사이즈로 나눠떨어지지 않는 경우, 마지막 배치를 버린다. 주로 학습시에만 사용."]},{"metadata":{"id":"YH-t5coUmuOM","colab_type":"text"},"cell_type":"markdown","source":["## Training neural network model\n","\n","\n","Training procedure\n","~~~~\n","for epoch in range(max_epoch):\n","    for input, target in dataset:    # retrieve input data and target labels\n","        optimizer.zero_grad()     # reset gradient\n","        output = model(input)     # forward propagation\n","        loss = loss_fn(output, target)  # get loss value\n","        loss.backward()           # back-propagation (compute gradient)\n","        optimizer.step()          # update parameters with gradient\n","~~~~"]},{"metadata":{"id":"FRw6ytc9DZCu","colab_type":"code","colab":{}},"cell_type":"code","source":["# utility function to measure time\n","import time\n","import math\n","\n","def timeSince(since):\n","    now = time.time()\n","    s = now - since\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"pEt4xYD4r0Lt","colab_type":"code","colab":{}},"cell_type":"code","source":["# set loss function and optimizer\n","loss_fn = nn.MSELoss()\n","optim = torch.optim.Adam(model.parameters(), lr=0.001)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"waSS2hrKr0Lv","colab_type":"text"},"cell_type":"markdown","source":["**nn.CrossEntropyLoss**: Cross entropy를 계산하는 Loss. softmax가 내부적으로 수행된다.\n","\n","**optim.Adam**: optim에는 여러 optimizer가 있고, Adam Optimizer는 대표적으로 많이 사용된다."]},{"metadata":{"id":"K4lF0ODwr0Lz","colab_type":"text"},"cell_type":"markdown","source":["### Training procedure\n","\n","첫번째 for문: 원하는 epoch만큼 반복\n","\n","두번째 for문: training datset에서 배치 사이즈 만큼씩 모두 샘플링 될 때까지 반복.\n","\n","**Line 2**: MNIST dataset은 DataLoader를 통해 image와 label을 return.\n","\n","**Line 4**: 각각 Device에 올린다 (GPU or CPU)\n","\n","**Line 5**: 모델에 이미지를 넣고 forward propagation 한다.\n","\n","**Line 7**: 결과값 y_hat과 실제 정답 y에 대한 loss를 계산한다.\n","\n","**zero_grad (Line 9)**: 모델의 gradient를 0으로 초기화한다.\n","\n","**backward (Line 10)**: loss를 계산하는 것까지 연결되어있는 graph를 따라 gradient를 계산한다.\n","\n","**step (Line 11)**: 계산된 gradient를 모두 parameter에 적용한다.\n","\n","**eval (Line 17)**: 모델을 evaluation mode로 바꿔준다 (dropout 조정, Batch normalization 조정 등)\n","\n","**torch.no_grad (Line 19)**: gradient를 계산하기 위해 추적하는 수고를 하지 않음\n","\n","**torch.max (Line 24)**: max value와 indices(즉, argmax)를 return.\n","\n","**train (Line 29)**: evaluation mode였던 모델을 train mode로 전환"]},{"metadata":{"id":"rVb4hicggYXY","colab_type":"code","colab":{}},"cell_type":"code","source":["# reset loss history\n","all_losses = []"],"execution_count":0,"outputs":[]},{"metadata":{"scrolled":true,"id":"-KAxJESir0L0","colab_type":"code","outputId":"cd94d883-e50e-4ecb-8fc0-9fbeb96dfd51","executionInfo":{"status":"ok","timestamp":1555377329993,"user_tz":-540,"elapsed":51182,"user":{"displayName":"Injung Kim","photoUrl":"","userId":"13406367766306429798"}},"colab":{"base_uri":"https://localhost:8080/","height":287}},"cell_type":"code","source":["max_epoch = 5        # maximum number of epochs\n","step = 0             # initialize step counter variable\n","\n","plot_every = 200\n","total_loss = 0 # Reset every plot_every iters\n","\n","start = time.time()\n","\n","for epoch in range(max_epoch):\n","    for idx, (images, labels) in enumerate(train_loader):\n","        x = images.to(DEVICE)       # (N, 1, 28, 28)\n","        \n","        x_hat = model(x)            # (N, 1, 28, 28)  # forward propagation\n","       \n","        loss = loss_fn(x_hat, x)  # computing loss\n","        total_loss += loss.item()\n","        \n","        optim.zero_grad()           # reset gradient\n","        loss.backward()             # back-propagation (compute gradient)\n","        optim.step()                # update parameters with gradient\n","        \n","        # periodically print loss\n","        if step % 500 == 0:\n","            print('Epoch({}): {}/{}, Step: {}, Loss: {}'.format(timeSince(start), epoch, max_epoch, step, loss.item()))\n","        \n","        if (step + 1) % plot_every == 0:\n","            all_losses.append(total_loss / plot_every)\n","            total_loss = 0\n","        \n","        # periodically evalute model on test data\n","        if step % 1000 == 0:\n","            model.eval()\n","            acc = 0.\n","            with torch.no_grad():   # disable autograd\n","                for idx, (images, labels) in enumerate(test_loader):\n","                    x, y = images.to(DEVICE), labels.to(DEVICE) # (N, 1, 28, 28), (N, )\n","                    x_hat = model(x) # (N, 10)\n","                    loss = loss_fn(x_hat, x)\n","                    \n","            print('*'*20, 'Test', '*'*20)\n","            print('Step: {}, Loss: {}'.format(step, loss.item()))\n","            print('*'*46)\n","            model.train()           # turn to train mode (enable autograd)\n","        step += 1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch(0m 0s): 0/5, Step: 0, Loss: 1.0285955667495728\n","******************** Test ********************\n","Step: 0, Loss: 0.9227064847946167\n","**********************************************\n","Epoch(0m 5s): 0/5, Step: 500, Loss: 0.08690234273672104\n","Epoch(0m 9s): 1/5, Step: 1000, Loss: 0.0668378621339798\n","******************** Test ********************\n","Step: 1000, Loss: 0.06496002525091171\n","**********************************************\n","Epoch(0m 14s): 1/5, Step: 1500, Loss: 0.05370214954018593\n","Epoch(0m 18s): 2/5, Step: 2000, Loss: 0.048125408589839935\n","******************** Test ********************\n","Step: 2000, Loss: 0.04663126543164253\n","**********************************************\n","Epoch(0m 23s): 2/5, Step: 2500, Loss: 0.041344981640577316\n"],"name":"stdout"}]},{"metadata":{"id":"ODLhr8mJhF0v","colab_type":"code","colab":{}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","\n","plt.figure()\n","plt.plot(all_losses)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ta_UwyT1r0L6","colab_type":"text"},"cell_type":"markdown","source":["## Test and Visualize"]},{"metadata":{"id":"6AwZsqeJr0L6","colab_type":"code","colab":{}},"cell_type":"code","source":["# Test\n","model.eval()\n","acc = 0.\n","with torch.no_grad():\n","    for idx, (images, labels) in enumerate(test_loader):\n","        x = images.to(DEVICE)        # (N, 1, 28, 28), (N, )\n","        x_hat = model(x) # (N, 10)\n","        loss = loss_fn(x_hat, x)\n","print('*'*20, 'Test', '*'*20)\n","print('Step: {}, Loss: {}'.format(step, loss.item()))\n","print('*'*46)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nFoQkM7Mr0L8","colab_type":"code","colab":{}},"cell_type":"code","source":["idx = 1234 # 0 to 9999\n","img, _ = mnist_test[idx]\n","img.shape"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qzrcr3vcr0L-","colab_type":"code","colab":{}},"cell_type":"code","source":["imshow(img[0], cmap='gray')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"E1qgio9Rr0MC","colab_type":"code","colab":{}},"cell_type":"code","source":["sample = img.to(DEVICE)\n","out = model(sample).cpu().detach().numpy()\n","out.shape\n","imshow(out[0,0], cmap='gray')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8cR6hl9hr0MF","colab_type":"code","colab":{}},"cell_type":"code","source":["# save parameters, if necessary.\n","torch.save(model.state_dict(), 'model.pkl')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Jo4QTwaQcR9m","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}